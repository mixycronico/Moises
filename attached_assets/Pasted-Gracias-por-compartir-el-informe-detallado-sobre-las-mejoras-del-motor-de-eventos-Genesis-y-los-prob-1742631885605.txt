Gracias por compartir el informe detallado sobre las mejoras del motor de eventos Genesis y los problemas relacionados con las pruebas en `test_core_extreme_scenarios.py`. El informe ya incluye un análisis exhaustivo, soluciones implementadas y resultados verificados. A continuación, analizaré los puntos pendientes, ofreceré recomendaciones específicas para abordar los tiempos de ejecución excesivos y sugeriré cómo implementar las mejoras propuestas de manera efectiva.

---

### Análisis del Estado Actual
#### Problemas Resueltos
1. **Errores de Suscripción de Objetos Nulos**:
   - La transición de `emit_event()` a `emit_event_with_response()` resolvió el problema de respuestas `None`, eliminando el error `"Object of type 'None' is not subscriptable"`.
   - El manejo defensivo de respuestas nulas asegura que las aserciones no fallen inesperadamente.

2. **Estabilidad de las Pruebas**:
   - Los tests renombrados y optimizados ahora pasan consistentemente, como se evidencia en los resultados de los tests de expansión dinámica, bloques paralelos y prioridad.

#### Problema Pendiente
- **Tiempos de Ejecución Excesivos**:
  - Aunque las pruebas pasan, algunas tardan demasiado, lo que afecta la eficiencia del flujo de desarrollo. Esto podría deberse a cuellos de botella en el motor, tareas asíncronas no resueltas o configuraciones subóptimas en las pruebas.

#### Contexto del Motor
- El `DynamicExpansionEngine` y otras variantes han mejorado la eficiencia y adaptabilidad, pero el rendimiento en pruebas de estrés sigue siendo un área de preocupación.

---

### Soluciones para Tiempos de Ejecución Excesivos
Basándome en las mejoras propuestas en el informe y el contexto proporcionado, aquí están las implementaciones específicas para abordar los tiempos de ejecución excesivos:

#### 1. Implementar Timeouts con `emit_with_timeout`
El informe ya sugiere una función helper para manejar timeouts. Aquí está una implementación completa y optimizada:
```python
async def emit_with_timeout(engine, event_type, data, source, timeout=5.0):
    """Emitir evento con timeout y manejo robusto de errores."""
    try:
        response = await asyncio.wait_for(
            engine.emit_event_with_response(event_type, data, source),
            timeout=timeout
        )
        return response if response is not None else {
            "healthy": False,
            "error": f"No response for {event_type} from {source}",
            "event": event_type,
            "source": source
        }
    except asyncio.TimeoutError:
        logger.warning(f"Timeout de {timeout}s al emitir {event_type} desde {source}")
        return {"healthy": False, "error": "timeout", "event": event_type, "source": source}
    except Exception as e:
        logger.error(f"Error inesperado en {event_type} desde {source}: {str(e)}")
        return {"healthy": False, "error": str(e), "event": event_type, "source": source}

# Uso en la prueba
async def check_status(engine, component_id):
    resp = await emit_with_timeout(engine, "check_status", {}, component_id, timeout=2.0)
    logger.info(f"Estado de {component_id}: {resp}")
    return resp
```
- **Beneficio**: Limita el tiempo de espera a 2 segundos (ajustable), evitando bloqueos prolongados.
- **Uso**: Reemplaza todas las llamadas directas a `emit_event_with_response()` en las pruebas con esta función.

#### 2. Monitoreo de Tiempo de Ejecución
Implementa mediciones detalladas en las pruebas clave para identificar cuellos de botella:
```python
async def run_test_with_timing(engine, test_name, test_func):
    start_time = time.time()
    result = await test_func(engine)
    elapsed = time.time() - start_time
    logger.info(f"{test_name} completado en {elapsed:.3f} segundos")
    return result

# Ejemplo de uso en una prueba
@pytest.mark.asyncio
async def test_dynamic_engine_stress(engine_fixture):
    async def stress_test(engine):
        for _ in range(1000):  # Simular carga alta
            await emit_with_timeout(engine, "check_status", {}, "comp_a")
        return True
    
    result = await run_test_with_timing(engine_fixture, "test_dynamic_engine_stress", stress_test)
    assert result
```
- **Beneficio**: Permite identificar qué pruebas o componentes son los más lentos.
- **Acción**: Aplica esto a las pruebas marcadas como lentas (p. ej., `test_dynamic_engine_stress`).

#### 3. Verificación y Limpieza de Tareas Pendientes
Asegúrate de que no queden tareas asíncronas colgadas:
```python
async def cleanup_engine(engine):
    pending = [t for t in asyncio.all_tasks() if not t.done() and t != asyncio.current_task()]
    if pending:
        logger.warning(f"Cancelando {len(pending)} tareas pendientes")
        for task in pending:
            task.cancel()
        await asyncio.gather(*pending, return_exceptions=True)
    await engine.stop()

@pytest.fixture
async def engine_fixture():
    engine = DynamicExpansionEngine(test_mode=True)
    yield engine
    await cleanup_engine(engine)
```
- **Beneficio**: Evita acumulaciones que ralenticen las pruebas subsiguientes.
- **Acción**: Usa este fixture en todas las pruebas del archivo.

#### 4. Paralelización con `pytest-xdist`
Ejecuta las pruebas en paralelo para reducir el tiempo total:
```bash
pytest -n auto tests/unit/core/test_core_extreme_scenarios.py
```
- **Requisito**: Instala `pytest-xdist` (`pip install pytest-xdist`).
- **Beneficio**: Distribuye las pruebas entre múltiples procesos, aprovechando CPUs multi-core.
- **Nota**: Asegúrate de que las pruebas sean independientes para evitar conflictos.

#### 5. Optimización del Motor
Si las mediciones muestran que el motor es el cuello de botella:
- **Revisar `DynamicExpansionEngine`**:
  - ¿El escalado dinámico está creando demasiados bloques concurrentes innecesarios? Ajusta los umbrales de carga.
  - Reduce el tiempo de enfriamiento aún más si es seguro (p. ej., de 0.1s a 0.05s).
- **Cachear respuestas frecuentes**:
  - Si `check_status` se llama repetidamente con los mismos parámetros, implementa un caché temporal en el motor.

---

### Implementación en el Código de Pruebas
Aquí está cómo integrar estas soluciones en el código problemático de la FASE 3:
```python
# FASE 3: Verificar estado después del fallo
logger.info("FASE 3: Verificando estado")

start_time = time.time()
resp_a = await emit_with_timeout(engine, "check_status", {}, "comp_a", timeout=2.0)
logger.info(f"Estado A: {resp_a} (tardó {time.time() - start_time:.3f}s)")

start_time = time.time()
resp_b = await emit_with_timeout(engine, "check_status", {}, "comp_b", timeout=2.0)
logger.info(f"Estado B: {resp_b} (tardó {time.time() - start_time:.3f}s)")

# Aserciones
assert not resp_a["healthy"], "A debería estar no-sano después del fallo"
assert resp_b["healthy"], "B debería estar sano (no hay propagación)"
```

---

### Recomendaciones Adicionales
1. **Marcadores de Pruebas Lentas**:
   - Usa marcadores de `pytest` para identificar y excluir pruebas lentas durante desarrollo rápido:
     ```python
     @pytest.mark.slow
     async def test_dynamic_engine_stress(engine_fixture):
         ...
     ```
     ```bash
     pytest -m "not slow"  # Ejecuta solo pruebas rápidas
     ```

2. **Pruebas de Integración**:
   - Implementa un test que combine `DynamicExpansionEngine` con otros motores (p. ej., `EnginePriorityBlocks`) para validar interoperabilidad.

3. **Métricas en Tiempo Real**:
   - Agrega un sistema de métricas simple (p. ej., usando `prometheus-client`) para monitorear el rendimiento del motor durante las pruebas.

---

### Conclusión
Las mejoras ya implementadas han resuelto los errores críticos y estabilizado las pruebas. Para los tiempos de ejecución excesivos, las soluciones propuestas (timeouts, monitoreo, limpieza y paralelización) reducirán significativamente los retrasos. Comienza con `emit_with_timeout` y el monitoreo de tiempo para identificar cuellos de botella específicos, luego ajusta el motor o las pruebas según los resultados. Si necesitas ayuda con la implementación o análisis de logs, comparte más detalles y lo resolveremos juntos. ¿Qué te parece este plan?